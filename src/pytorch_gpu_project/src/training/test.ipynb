{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "from timm.loss import BinaryCrossEntropy\n",
    "from timm.optim import create_optimizer_v2\n",
    "import torch\n",
    "from pytorch_accelerated.callbacks import SaveBestModelCallback\n",
    "from pytorch_accelerated.trainer import DEFAULT_CALLBACKS\n",
    "\n",
    "from src.data.datasets.coin_data import CoinData, CoinDataFolder\n",
    "from src.training.trainer import TimmMixupTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.is_available(): True\n",
      "torch.cuda.device_count(): 1\n",
      "torch.backends.mkl.is_available(): True\n",
      "torch.backends.cudnn.is_available(): True\n",
      "torch.backends.cuda.is_built(): True\n",
      "torch.backends.mkldnn.is_available(): True\n"
     ]
    }
   ],
   "source": [
    "print(\"torch.cuda.is_available():\", torch.cuda.is_available())\n",
    "print(\"torch.cuda.device_count():\", torch.cuda.device_count())\n",
    "print(\"torch.backends.mkl.is_available():\", torch.backends.mkl.is_available())\n",
    "print(\"torch.backends.cudnn.is_available():\", torch.backends.cudnn.is_available())\n",
    "print(\"torch.backends.cuda.is_built():\", torch.backends.cuda.is_built())\n",
    "print(\"torch.backends.mkldnn.is_available():\", torch.backends.mkldnn.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Enable autoreloading of imported modules.\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training arguments, hardcoded here for clarity\n",
    "image_size = (224, 224)\n",
    "lr = 5e-3\n",
    "smoothing = 0.1\n",
    "mixup = 0.2\n",
    "cutmix = 1.0\n",
    "batch_size = 16\n",
    "bce_target_thresh = 0.2\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/raw/CN_dataset_04_23/data_types_example\n",
      "[('../data/raw/CN_dataset_04_23/data_types_example/1/CN_type_1_cn_coin_8022_p.jpg', 0), ('../data/raw/CN_dataset_04_23/data_types_example/1/CN_type_1_MK_18203122_cn_coin_6383_o.jpg', 0), ('../data/raw/CN_dataset_04_23/data_types_example/2/CN_type_2_cn_coin_8024_p.jpg', 1), ('../data/raw/CN_dataset_04_23/data_types_example/3/CN_type_3_BNF_Platzhalter_cn_coin_11904_o.jpg', 2), ('../data/raw/CN_dataset_04_23/data_types_example/3/CN_type_3_MK_18247614_cn_coin_6696_o.jpg', 2), ('../data/raw/CN_dataset_04_23/data_types_example/5/CN_type_5_cn_coin_7685_p.jpg', 3), ('../data/raw/CN_dataset_04_23/data_types_example/6/CN_type_6_cn_coin_7686_p.jpg', 4), ('../data/raw/CN_dataset_04_23/data_types_example/8/CN_type_8_cn_coin_7689_p.jpg', 5), ('../data/raw/CN_dataset_04_23/data_types_example/8/CN_type_8_cn_coin_15352_p.jpg', 5), ('../data/raw/CN_dataset_04_23/data_types_example/11/CN_type_11_cn_coin_8036_p.jpg', 6), ('../data/raw/CN_dataset_04_23/data_types_example/11/CN_type_11_MK_18247626_cn_coin_8036_o.jpg', 6), ('../data/raw/CN_dataset_04_23/data_types_example/13/CN_type_13_BNF_FRBNF41826912_cn_coin_7693_o.jpg', 7), ('../data/raw/CN_dataset_04_23/data_types_example/13/CN_type_13_cn_coin_7692_p.jpg', 7), ('../data/raw/CN_dataset_04_23/data_types_example/14/CN_type_14_MK_18247628_cn_coin_6701_o.jpg', 8), ('../data/raw/CN_dataset_04_23/data_types_example/15/CN_type_15_BNF_FRBNF41826908_cn_coin_11907_o.jpg', 9), ('../data/raw/CN_dataset_04_23/data_types_example/15/CN_type_15_cn_coin_7695_p.jpg', 9), ('../data/raw/CN_dataset_04_23/data_types_example/17/CN_type_17_cn_coin_7698_p.jpg', 10), ('../data/raw/CN_dataset_04_23/data_types_example/18/CN_type_18_cn_coin_7700_p.jpg', 11), ('../data/raw/CN_dataset_04_23/data_types_example/19/CN_type_19_cn_coin_7701_p.jpg', 12), ('../data/raw/CN_dataset_04_23/data_types_example/20/CN_type_20_cn_coin_7702_p.jpg', 13), ('../data/raw/CN_dataset_04_23/data_types_example/21/CN_type_21_BNF_582_cn_coin_8025_o.jpg', 14), ('../data/raw/CN_dataset_04_23/data_types_example/21/CN_type_21_cn_coin_8025_p.jpg', 14), ('../data/raw/CN_dataset_04_23/data_types_example/21/CN_type_21_cn_coin_8026_p.jpg', 14), ('../data/raw/CN_dataset_04_23/data_types_example/21/CN_type_21_MK_18239074_cn_coin_6426_o.jpg', 14), ('../data/raw/CN_dataset_04_23/data_types_example/21/CN_type_21_MK_18239075_cn_coin_6427_o.jpg', 14), ('../data/raw/CN_dataset_04_23/data_types_example/21/CN_type_21_MK_18239247_cn_coin_6428_o.jpg', 14), ('../data/raw/CN_dataset_04_23/data_types_example/22/CN_type_22_cn_coin_8023_p.jpg', 15), ('../data/raw/CN_dataset_04_23/data_types_example/23/CN_type_23_BNF_FRBNF41826909_cn_coin_11908_o.jpg', 16), ('../data/raw/CN_dataset_04_23/data_types_example/23/CN_type_23_cn_coin_7704_p.jpg', 16), ('../data/raw/CN_dataset_04_23/data_types_example/23/CN_type_23_MK_18247633_cn_coin_6704_o.jpg', 16), ('../data/raw/CN_dataset_04_23/data_types_example/24/CN_type_24_cn_coin_7703_p.jpg', 17), ('../data/raw/CN_dataset_04_23/data_types_example/25/CN_type_25_cn_coin_8030_p.jpg', 18), ('../data/raw/CN_dataset_04_23/data_types_example/26/CN_type_26_cn_coin_8033_p.jpg', 19), ('../data/raw/CN_dataset_04_23/data_types_example/27/CN_type_27_cn_coin_7705_p.jpg', 20), ('../data/raw/CN_dataset_04_23/data_types_example/30/CN_type_30_cn_coin_7707_p.jpg', 21), ('../data/raw/CN_dataset_04_23/data_types_example/32/CN_type_32_cn_coin_7711_p.jpg', 22), ('../data/raw/CN_dataset_04_23/data_types_example/33/CN_type_33_cn_coin_7712_p.jpg', 23), ('../data/raw/CN_dataset_04_23/data_types_example/34/CN_type_34_cn_coin_7713_p.jpg', 24), ('../data/raw/CN_dataset_04_23/data_types_example/34/CN_type_34_MK_18247693_cn_coin_6708_o.jpg', 24), ('../data/raw/CN_dataset_04_23/data_types_example/35/CN_type_35_MK_18247690_cn_coin_6707_o.jpg', 25), ('../data/raw/CN_dataset_04_23/data_types_example/37/CN_type_37_BNF_FRBNF41826913_cn_coin_11912_o.jpg', 26), ('../data/raw/CN_dataset_04_23/data_types_example/37/CN_type_37_cn_coin_7717_p.jpg', 26), ('../data/raw/CN_dataset_04_23/data_types_example/40/CN_type_40_cn_coin_7720_p.jpg', 27), ('../data/raw/CN_dataset_04_23/data_types_example/41/CN_type_41_cn_coin_8029_p.jpg', 28), ('../data/raw/CN_dataset_04_23/data_types_example/41/CN_type_41_cn_coin_8031_p.jpg', 28), ('../data/raw/CN_dataset_04_23/data_types_example/41/CN_type_41_cn_coin_8032_p.jpg', 28), ('../data/raw/CN_dataset_04_23/data_types_example/42/CN_type_42_cn_coin_8039_p.jpg', 29), ('../data/raw/CN_dataset_04_23/data_types_example/42/CN_type_42_cn_coin_8040_p.jpg', 29), ('../data/raw/CN_dataset_04_23/data_types_example/42/CN_type_42_cn_coin_8041_p.jpg', 29), ('../data/raw/CN_dataset_04_23/data_types_example/42/CN_type_42_cn_coin_8042_p.jpg', 29), ('../data/raw/CN_dataset_04_23/data_types_example/42/CN_type_42_cn_coin_8043_p.jpg', 29), ('../data/raw/CN_dataset_04_23/data_types_example/42/CN_type_42_cn_coin_8044_p.jpg', 29), ('../data/raw/CN_dataset_04_23/data_types_example/42/CN_type_42_MK_18235358_cn_coin_5614_o.jpg', 29), ('../data/raw/CN_dataset_04_23/data_types_example/42/CN_type_42_MK_18239453_cn_coin_6441_o.jpg', 29), ('../data/raw/CN_dataset_04_23/data_types_example/45/CN_type_45_cn_coin_8046_p.jpg', 30), ('../data/raw/CN_dataset_04_23/data_types_example/46/CN_type_46_BNF_Platzhalter_cn_coin_8050_o.jpg', 31), ('../data/raw/CN_dataset_04_23/data_types_example/46/CN_type_46_cn_coin_8047_p.jpg', 31), ('../data/raw/CN_dataset_04_23/data_types_example/46/CN_type_46_cn_coin_8048_p.jpg', 31), ('../data/raw/CN_dataset_04_23/data_types_example/46/CN_type_46_cn_coin_8049_p.jpg', 31), ('../data/raw/CN_dataset_04_23/data_types_example/46/CN_type_46_cn_coin_8050_p.jpg', 31), ('../data/raw/CN_dataset_04_23/data_types_example/46/CN_type_46_cn_coin_8053_p.jpg', 31), ('../data/raw/CN_dataset_04_23/data_types_example/46/CN_type_46_cn_coin_8054_p.jpg', 31), ('../data/raw/CN_dataset_04_23/data_types_example/46/CN_type_46_cn_coin_8055_p.jpg', 31), ('../data/raw/CN_dataset_04_23/data_types_example/46/CN_type_46_MK_18239360_cn_coin_6439_o.jpg', 31), ('../data/raw/CN_dataset_04_23/data_types_example/46/CN_type_46_MK_18239363_cn_coin_6440_o.jpg', 31), ('../data/raw/CN_dataset_04_23/data_types_example/47/CN_type_47_cn_coin_7725_p.jpg', 32), ('../data/raw/CN_dataset_04_23/data_types_example/48/CN_type_48_cn_coin_7726_p.jpg', 33), ('../data/raw/CN_dataset_04_23/data_types_example/49/CN_type_49_cn_coin_8056_p.jpg', 34), ('../data/raw/CN_dataset_04_23/data_types_example/50/CN_type_50_BNF_604_cn_coin_7727_o.jpg', 35), ('../data/raw/CN_dataset_04_23/data_types_example/50/CN_type_50_cn_coin_7727_p.jpg', 35), ('../data/raw/CN_dataset_04_23/data_types_example/51/CN_type_51_BNF_1370_cn_coin_8077_o.jpg', 36), ('../data/raw/CN_dataset_04_23/data_types_example/51/CN_type_51_BNF_1371_cn_coin_8071_o.jpg', 36), ('../data/raw/CN_dataset_04_23/data_types_example/51/CN_type_51_BNF_M 2608_cn_coin_8070_o.jpg', 36), ('../data/raw/CN_dataset_04_23/data_types_example/51/CN_type_51_BNF_R 1364_cn_coin_8076_o.jpg', 36), ('../data/raw/CN_dataset_04_23/data_types_example/51/CN_type_51_cn_coin_8059_p.jpg', 36), ('../data/raw/CN_dataset_04_23/data_types_example/51/CN_type_51_cn_coin_8061_p.jpg', 36), ('../data/raw/CN_dataset_04_23/data_types_example/51/CN_type_51_cn_coin_8062_p.jpg', 36), ('../data/raw/CN_dataset_04_23/data_types_example/51/CN_type_51_cn_coin_8063_p.jpg', 36), ('../data/raw/CN_dataset_04_23/data_types_example/51/CN_type_51_cn_coin_8064_p.jpg', 36), ('../data/raw/CN_dataset_04_23/data_types_example/51/CN_type_51_cn_coin_8066_p.jpg', 36), ('../data/raw/CN_dataset_04_23/data_types_example/51/CN_type_51_cn_coin_8067_p.jpg', 36), ('../data/raw/CN_dataset_04_23/data_types_example/51/CN_type_51_cn_coin_8068_p.jpg', 36), ('../data/raw/CN_dataset_04_23/data_types_example/51/CN_type_51_cn_coin_8069_p.jpg', 36), ('../data/raw/CN_dataset_04_23/data_types_example/51/CN_type_51_cn_coin_8070_p.jpg', 36), ('../data/raw/CN_dataset_04_23/data_types_example/51/CN_type_51_cn_coin_8071_p.jpg', 36), ('../data/raw/CN_dataset_04_23/data_types_example/51/CN_type_51_cn_coin_8072_p.jpg', 36), ('../data/raw/CN_dataset_04_23/data_types_example/51/CN_type_51_cn_coin_8073_p.jpg', 36), ('../data/raw/CN_dataset_04_23/data_types_example/51/CN_type_51_cn_coin_8074_p.jpg', 36), ('../data/raw/CN_dataset_04_23/data_types_example/51/CN_type_51_cn_coin_8075_p.jpg', 36), ('../data/raw/CN_dataset_04_23/data_types_example/51/CN_type_51_cn_coin_8076_p.jpg', 36), ('../data/raw/CN_dataset_04_23/data_types_example/51/CN_type_51_cn_coin_8077_p.jpg', 36), ('../data/raw/CN_dataset_04_23/data_types_example/51/CN_type_51_cn_coin_8078_p.jpg', 36), ('../data/raw/CN_dataset_04_23/data_types_example/51/CN_type_51_MK_18239253_cn_coin_6431_o.jpg', 36), ('../data/raw/CN_dataset_04_23/data_types_example/51/CN_type_51_MK_18239255_cn_coin_6432_o.jpg', 36), ('../data/raw/CN_dataset_04_23/data_types_example/51/CN_type_51_MK_18239354_cn_coin_6435_o.jpg', 36), ('../data/raw/CN_dataset_04_23/data_types_example/51/CN_type_51_MK_18239355_cn_coin_6436_o.jpg', 36), ('../data/raw/CN_dataset_04_23/data_types_example/52/CN_type_52_BNF_Platzhalter_cn_coin_11915_o.jpg', 37), ('../data/raw/CN_dataset_04_23/data_types_example/54/CN_type_54_cn_coin_8087_p.jpg', 38), ('../data/raw/CN_dataset_04_23/data_types_example/54/CN_type_54_cn_coin_8088_p.jpg', 38), ('../data/raw/CN_dataset_04_23/data_types_example/54/CN_type_54_cn_coin_8089_p.jpg', 38), ('../data/raw/CN_dataset_04_23/data_types_example/54/CN_type_54_MK_18239352_cn_coin_6433_o.jpg', 38), ('../data/raw/CN_dataset_04_23/data_types_example/54/CN_type_54_MK_18239358_cn_coin_6437_o.jpg', 38), ('../data/raw/CN_dataset_04_23/data_types_example/54/CN_type_54_MK_18239359_cn_coin_6438_o.jpg', 38), ('../data/raw/CN_dataset_04_23/data_types_example/55/CN_type_55_BNF_Platzhalter_cn_coin_11916_o.jpg', 39), ('../data/raw/CN_dataset_04_23/data_types_example/56/CN_type_56_cn_coin_15458_p.jpg', 40), ('../data/raw/CN_dataset_04_23/data_types_example/58/CN_type_58_BNF_419_cn_coin_7759_o.jpg', 41), ('../data/raw/CN_dataset_04_23/data_types_example/58/CN_type_58_cn_coin_7759_p.jpg', 41), ('../data/raw/CN_dataset_04_23/data_types_example/59/CN_type_59_BNF_Platzhalter_cn_coin_11923_o.jpg', 42), ('../data/raw/CN_dataset_04_23/data_types_example/60/CN_type_60_BNF_1966.453_cn_coin_11902_o.jpg', 43), ('../data/raw/CN_dataset_04_23/data_types_example/60/CN_type_60_BNF_FRBNF41826921_cn_coin_11920_o.jpg', 43), ('../data/raw/CN_dataset_04_23/data_types_example/60/CN_type_60_cn_coin_7738_p.jpg', 43), ('../data/raw/CN_dataset_04_23/data_types_example/60/CN_type_60_cn_coin_7739_p.jpg', 43), ('../data/raw/CN_dataset_04_23/data_types_example/60/CN_type_60_cn_coin_7741_p.jpg', 43), ('../data/raw/CN_dataset_04_23/data_types_example/60/CN_type_60_MK_18247707_cn_coin_6710_o.jpg', 43), ('../data/raw/CN_dataset_04_23/data_types_example/60/CN_type_60_MK_18247708_cn_coin_6711_o.jpg', 43), ('../data/raw/CN_dataset_04_23/data_types_example/61/CN_type_61_BNF_FRBNF41826920_cn_coin_7742_o.jpg', 44), ('../data/raw/CN_dataset_04_23/data_types_example/64/CN_type_64_cn_coin_7745_p.jpg', 45), ('../data/raw/CN_dataset_04_23/data_types_example/64/CN_type_64_MK_18247714_cn_coin_10340_o.jpg', 45), ('../data/raw/CN_dataset_04_23/data_types_example/65/CN_type_65_cn_coin_7746_p.jpg', 46), ('../data/raw/CN_dataset_04_23/data_types_example/65/CN_type_65_MK_18247711_cn_coin_6713_o.jpg', 46), ('../data/raw/CN_dataset_04_23/data_types_example/72/CN_type_72_BNF_FRBNF41826919_cn_coin_11918_o.jpg', 47), ('../data/raw/CN_dataset_04_23/data_types_example/73/CN_type_73_cn_coin_7760_p.jpg', 48), ('../data/raw/CN_dataset_04_23/data_types_example/74/CN_type_74_BNF_Platzhalter_cn_coin_11925_o.jpg', 49), ('../data/raw/CN_dataset_04_23/data_types_example/74/CN_type_74_MK_18247710_cn_coin_6712_o.jpg', 49), ('../data/raw/CN_dataset_04_23/data_types_example/75/CN_type_75_MK_18247728_cn_coin_7765_o.jpg', 50), ('../data/raw/CN_dataset_04_23/data_types_example/77/CN_type_77_BNF_FRBNF41826930_cn_coin_11929_o.jpg', 51), ('../data/raw/CN_dataset_04_23/data_types_example/77/CN_type_77_cn_coin_7769_p.jpg', 51), ('../data/raw/CN_dataset_04_23/data_types_example/77/CN_type_77_MK_18247712_cn_coin_6714_o.jpg', 51), ('../data/raw/CN_dataset_04_23/data_types_example/81/CN_type_81_BNF_FRBNF41826932_cn_coin_11931_o.jpg', 52), ('../data/raw/CN_dataset_04_23/data_types_example/81/CN_type_81_BNF_Platzhalter_cn_coin_11932_o.jpg', 52), ('../data/raw/CN_dataset_04_23/data_types_example/81/CN_type_81_BNF_Platzhalter_cn_coin_11933_o.jpg', 52), ('../data/raw/CN_dataset_04_23/data_types_example/81/CN_type_81_cn_coin_7778_p.jpg', 52), ('../data/raw/CN_dataset_04_23/data_types_example/81/CN_type_81_cn_coin_7779_p.jpg', 52), ('../data/raw/CN_dataset_04_23/data_types_example/81/CN_type_81_MK_18247737_cn_coin_10348_o.jpg', 52), ('../data/raw/CN_dataset_04_23/data_types_example/81/CN_type_81_MK_18247740_cn_coin_10349_o.jpg', 52), ('../data/raw/CN_dataset_04_23/data_types_example/81/CN_type_81_MK_18247849_cn_coin_10354_o.jpg', 52), ('../data/raw/CN_dataset_04_23/data_types_example/81/CN_type_81_MK_18248051_cn_coin_6735_o.jpg', 52), ('../data/raw/CN_dataset_04_23/data_types_example/83/CN_type_83_MK_18247734_cn_coin_6715_o.jpg', 53), ('../data/raw/CN_dataset_04_23/data_types_example/84/CN_type_84_cn_coin_7782_p.jpg', 54), ('../data/raw/CN_dataset_04_23/data_types_example/85/CN_type_85_cn_coin_7783_p.jpg', 55), ('../data/raw/CN_dataset_04_23/data_types_example/85/CN_type_85_MK_18247852_cn_coin_10357_o.jpg', 55), ('../data/raw/CN_dataset_04_23/data_types_example/87/CN_type_87_BNF_Platzhalter_cn_coin_11938_o.jpg', 56), ('../data/raw/CN_dataset_04_23/data_types_example/87/CN_type_87_cn_coin_7787_p.jpg', 56), ('../data/raw/CN_dataset_04_23/data_types_example/87/CN_type_87_MK_18247859_cn_coin_10360_o.jpg', 56), ('../data/raw/CN_dataset_04_23/data_types_example/88/CN_type_88_BNF_Platzhalter_cn_coin_11939_o.jpg', 57), ('../data/raw/CN_dataset_04_23/data_types_example/88/CN_type_88_cn_coin_7788_p.jpg', 57), ('../data/raw/CN_dataset_04_23/data_types_example/90/CN_type_90_cn_coin_7791_p.jpg', 58), ('../data/raw/CN_dataset_04_23/data_types_example/90/CN_type_90_MK_18247936_cn_coin_10373_o.jpg', 58), ('../data/raw/CN_dataset_04_23/data_types_example/92/CN_type_92_MK_18247887_cn_coin_18743_o.jpg', 59), ('../data/raw/CN_dataset_04_23/data_types_example/95/CN_type_95_BNF_FRBNF41826937_cn_coin_11936_o.jpg', 60), ('../data/raw/CN_dataset_04_23/data_types_example/97/CN_type_97_cn_coin_7795_p.jpg', 61), ('../data/raw/CN_dataset_04_23/data_types_example/97/CN_type_97_MK_18247928_cn_coin_10368_o.jpg', 61), ('../data/raw/CN_dataset_04_23/data_types_example/97/CN_type_97_MK_18247933_cn_coin_10371_o.jpg', 61), ('../data/raw/CN_dataset_04_23/data_types_example/98/CN_type_98_BNF_536_cn_coin_7798_o.jpg', 62), ('../data/raw/CN_dataset_04_23/data_types_example/99/CN_type_99_cn_coin_7799_p.jpg', 63), ('../data/raw/CN_dataset_04_23/data_types_example/100/CN_type_100_cn_coin_7800_p.jpg', 64)]\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "coin_data = CoinData()\n",
    "num_classes = len(coin_data.images_and_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixup_args = dict(\n",
    "    mixup_alpha=mixup,\n",
    "    cutmix_alpha=cutmix,\n",
    "    label_smoothing=smoothing,\n",
    "    num_classes=num_classes,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model using timm\n",
    "model = timm.create_model(\n",
    "    \"resnet34\", pretrained=False, num_classes=num_classes, #drop_path_rate=0.05\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data config associated with the model to use in data augmentation pipeline\n",
    "data_config = timm.data.resolve_data_config({}, model=model, verbose=True)\n",
    "data_mean = data_config[\"mean\"]\n",
    "data_std = data_config[\"std\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, eval_dataset = coin_data.generate_train_val_datasets(val_pct=0.3, image_size=image_size, data_mean=data_mean, data_std=data_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "          ...,\n",
       "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
       " \n",
       "         [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "          ...,\n",
       "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
       " \n",
       "         [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "          ...,\n",
       "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]]),\n",
       " 52)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimizer\n",
    "optimizer = create_optimizer_v2(\n",
    "    model, opt=\"RMSprop\", lr=lr, weight_decay=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RMSprop (\n",
       "Parameter Group 0\n",
       "    alpha: 0.9\n",
       "    centered: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    lr: 0.005\n",
       "    maximize: False\n",
       "    momentum: 0.9\n",
       "    weight_decay: 0.0\n",
       "\n",
       "Parameter Group 1\n",
       "    alpha: 0.9\n",
       "    centered: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    lr: 0.005\n",
       "    maximize: False\n",
       "    momentum: 0.9\n",
       "    weight_decay: 0.01\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we are using Mixup, we can use BCE during training and CE for evaluation\n",
    "train_loss_fn = BinaryCrossEntropy(\n",
    "    target_threshold=bce_target_thresh, smoothing=smoothing\n",
    ")\n",
    "validate_loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer and start training\n",
    "trainer = TimmMixupTrainer(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    loss_func=train_loss_fn,\n",
    "    eval_loss_fn=validate_loss_fn,\n",
    "    mixup_args=mixup_args,\n",
    "    num_classes=num_classes,\n",
    "    callbacks=[\n",
    "        *DEFAULT_CALLBACKS,\n",
    "        SaveBestModelCallback(watch_metric=\"accuracy\", greater_is_better=True),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trainer.train(\\n        per_device_batch_size=batch_size,\\n        train_dataset=train_dataset,\\n        eval_dataset=eval_dataset,\\n        num_epochs=num_epochs,\\n        create_scheduler_fn=trainer.create_scheduler,\\n    )'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"trainer.train(\n",
    "        per_device_batch_size=batch_size,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        num_epochs=num_epochs,\n",
    "        create_scheduler_fn=trainer.create_scheduler,\n",
    "    )\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting evaluation run\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1 trainer.evaluate(dataset=eval_dataset)                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.9/site-packages/pytorch_accelerated/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">512</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">evaluate</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 509 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 510 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._check_eval_batch_size()                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 511 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 512 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._run_evaluation()                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 513 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 514 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">get_default_train_dl_kwargs</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, batch_size) -&gt; <span style=\"color: #00ffff; text-decoration-color: #00ffff\">dict</span>:                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 515 </span><span style=\"color: #bfbfbf; text-decoration-color: #bfbfbf\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"</span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.9/site-packages/pytorch_accelerated/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">760</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_run_evaluation</span>     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 757 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>,                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 758 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 759 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 760 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._run_eval_epoch(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._eval_dataloader, is_training=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">False</span>)                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 761 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">except</span> StopTrainingError <span style=\"color: #0000ff; text-decoration-color: #0000ff\">as</span> e:                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 762 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._accelerator.print(e)                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 763 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.callback_handler.call_event(                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.9/site-packages/pytorch_accelerated/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">901</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_run_eval_epoch</span>     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 898 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"on_eval_step_start\"</span>,                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 899 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>,                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 900 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>)                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 901 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>batch_output = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.calculate_eval_batch_loss(batch)                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 902 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 903 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._update_loss_tracker(                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 904 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   </span>batch_output[<span style=\"color: #808000; text-decoration-color: #808000\">\"loss\"</span>], batch_output[<span style=\"color: #808000; text-decoration-color: #808000\">\"batch_size\"</span>]                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/workspace/src/pytorch_gpu_project/src/training/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">72</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">calculate_eval_batch_loss</span>       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">69 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>val_loss = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.eval_loss_fn(outputs, yb)                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">70 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.accuracy.update(outputs.argmax(-<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>), yb)                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">71 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>72 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>ema_model_preds = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.ema_model.module(xb).argmax(-<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>)                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">73 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.ema_accuracy.update(ema_model_preds, yb)                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">74 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">75 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> {<span style=\"color: #808000; text-decoration-color: #808000\">\"loss\"</span>: val_loss, <span style=\"color: #808000; text-decoration-color: #808000\">\"model_outputs\"</span>: outputs, <span style=\"color: #808000; text-decoration-color: #808000\">\"batch_size\"</span>: xb.size(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>)}       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">AttributeError: </span><span style=\"color: #008000; text-decoration-color: #008000\">'NoneType'</span> object has no attribute <span style=\"color: #008000; text-decoration-color: #008000\">'module'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m1\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1 trainer.evaluate(dataset=eval_dataset)                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.9/site-packages/pytorch_accelerated/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m512\u001b[0m in \u001b[92mevaluate\u001b[0m            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 509 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 510 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m._check_eval_batch_size()                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 511 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 512 \u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m._run_evaluation()                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 513 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 514 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mget_default_train_dl_kwargs\u001b[0m(\u001b[96mself\u001b[0m, batch_size) -> \u001b[96mdict\u001b[0m:                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 515 \u001b[0m\u001b[2;90m│   │   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.9/site-packages/pytorch_accelerated/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m760\u001b[0m in \u001b[92m_run_evaluation\u001b[0m     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 757 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m,                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 758 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 759 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 760 \u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m._run_eval_epoch(\u001b[96mself\u001b[0m._eval_dataloader, is_training=\u001b[94mFalse\u001b[0m)                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 761 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mexcept\u001b[0m StopTrainingError \u001b[94mas\u001b[0m e:                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 762 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m._accelerator.print(e)                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 763 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m.callback_handler.call_event(                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.9/site-packages/pytorch_accelerated/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m901\u001b[0m in \u001b[92m_run_eval_epoch\u001b[0m     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 898 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mon_eval_step_start\u001b[0m\u001b[33m\"\u001b[0m,                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 899 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0m\u001b[96mself\u001b[0m,                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 900 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m)                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 901 \u001b[2m│   │   │   │   │   \u001b[0mbatch_output = \u001b[96mself\u001b[0m.calculate_eval_batch_loss(batch)                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 902 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 903 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[96mself\u001b[0m._update_loss_tracker(                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 904 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0mbatch_output[\u001b[33m\"\u001b[0m\u001b[33mloss\u001b[0m\u001b[33m\"\u001b[0m], batch_output[\u001b[33m\"\u001b[0m\u001b[33mbatch_size\u001b[0m\u001b[33m\"\u001b[0m]                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/workspace/src/pytorch_gpu_project/src/training/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m72\u001b[0m in \u001b[92mcalculate_eval_batch_loss\u001b[0m       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m69 \u001b[0m\u001b[2m│   │   │   \u001b[0mval_loss = \u001b[96mself\u001b[0m.eval_loss_fn(outputs, yb)                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m70 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m.accuracy.update(outputs.argmax(-\u001b[94m1\u001b[0m), yb)                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m71 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m72 \u001b[2m│   │   │   \u001b[0mema_model_preds = \u001b[96mself\u001b[0m.ema_model.module(xb).argmax(-\u001b[94m1\u001b[0m)                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m73 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m.ema_accuracy.update(ema_model_preds, yb)                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m74 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m75 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m {\u001b[33m\"\u001b[0m\u001b[33mloss\u001b[0m\u001b[33m\"\u001b[0m: val_loss, \u001b[33m\"\u001b[0m\u001b[33mmodel_outputs\u001b[0m\u001b[33m\"\u001b[0m: outputs, \u001b[33m\"\u001b[0m\u001b[33mbatch_size\u001b[0m\u001b[33m\"\u001b[0m: xb.size(\u001b[94m0\u001b[0m)}       \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mAttributeError: \u001b[0m\u001b[32m'NoneType'\u001b[0m object has no attribute \u001b[32m'module'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.evaluate(dataset=eval_dataset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(f=\"../training/best_model.pt\", map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model_state_dict', 'optimizer_state_dict', 'loss'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1739)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint[\"loss\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
